---
title: "29天，从5个独立应用到统一平台"
date: "2026-02-18"
summary: "第2篇（共3篇）：PanPanMao的技术构建全过程，分为4个阶段——monorepo整合、测试与品牌、商业化、里程碑冲刺。1,134次commit，85个端点，9个产品线。"
tags: ["AI", "Software Development", "LLM", "Agents"]
series: "盘盘猫的故事"
part: 2
type: "Post"
status: "Published"
---

在第1篇里，我解释了为什么要在零领域知识的情况下做一个算命平台。现在来说说它到底是怎么做出来的——不是精选集锦，而是真实的架构决策、最棘手的bug，以及让每天39个commit成为可能的基础设施。

## 第一阶段：五合一（第1-4天）

起步时有5个独立的Next.js应用，放在5个独立的仓库里。八字、星座、塔罗、解梦、占卜。每个都有自己的认证、自己的Supabase客户端、自己的Tailwind配置。用户得登录五次。

**第1天**搭Turborepo。21个commit。概念上很简单——把应用搬到`apps/`，共享代码提取到`packages/`——但每条import路径都炸了。第4步和第5步（修import、验证构建）占了80%的时间。

**第2天**是CSS地狱。当我把Tailwind配置统一到`packages/config`后，每个依赖自定义design token的应用都崩了。不是那种明显的"红色报错"，而是那种微妙的"为什么这个按钮小了2px"。38个commit，每个都在修一个视觉回归。

但第2天也产出了第一批真正的成果：提取了四个共享package——PDF生成、聊天记录、共享聊天UI和通用React hooks。看着5个应用里的重复代码收敛成一个单一数据源，感觉极其爽。

**第3天**加了MBTI性格测试，成为第6个应用。还做了微信浏览器检测——这是关键的一环，因为中国互联网流量中有很大一部分来自微信内置浏览器。UA字符串的嗅探需要做加固，因为微信的UA在Android、iOS和桌面端各不相同。

**第4天**是56个commit——统一API package。`packages/api`标准化了所有应用与AI模型的通信方式。52个API路由被迁移到统一模式：认证检查、积分扣除、模型选择、SSE流式输出、错误处理。Claude Code看了一个示例后就迁移了全部52个。实际耗时一个小时，手动做至少一整天。

第4天还上线了：GDPR合规、中英双语法律页面，以及第一版基于DFA的敏感词过滤器。这个过滤器至关重要——玄学内容天然包含一些会触发简单关键词过滤的词汇（算命、破财、桃花劫）。我需要的是一套合理的白名单方案。

## 第二阶段：测试、品牌和那只猫（第5-10天）

### 测试冲刺

全monorepo用Vitest。优先级：

1. **先测共享package** —— API流式输出helper、Supabase客户端、积分管理。这些是承重墙。

1. **再测领域计算逻辑** —— 八字计算、塔罗抽牌、六壬占卜。我无法亲自验证正确性（零领域知识），但可以验证确定性输入产出预期输出。

1. **最后测API路由** —— 用mock认证token做集成测试。

GitHub Actions CI：每个PR都跑lint、类型检查、构建、测试。到我开始正式里程碑时，测试套件已经增长到34个文件中的1,922个测试。

### 品牌

PanPanMao（盼盼猫）。会算命的猫。一旦"猫"这个主题确定了，一切都顺理成章。每个应用都有了猫主题的装饰。积分货币变成了小鱼干。你不拿小鱼干付，拿什么付给一只猫呢？

### Stripe

四个美元积分套餐：10条/$6.99、50条/$24.99、150条/$59.99（推荐）、500条/$149.99。积分抽象层很关键——人们更愿意花5条小鱼干，而不是花$0.70。还预置了未激活的人民币套餐，为中国市场上线做准备。

## 第三阶段：硬核工程（第12-19天）

### 相术：浏览器端ML的手面相识别

技术难度最高的功能。PR #1讲述了这个故事：56个文件变更，6,881行新增。

**MediaPipe完全运行在浏览器端。** 检测不需要服务端往返。hooks架构如下：

- `useMediaPipeLoader` —— 懒加载WASM（约3MB），模块级缓存

- `useFaceDetector` —— 实时BlazeFace，60fps用于摄像头预览

- `useFaceLandmarker` —— 478点面部网格，拍照时运行一次

- `useHandLandmarker` —— 21点手部关键点，拍照时运行一次

PR #50后来把WASM和模型文件本地化到`public/mediapipe/`下的应用托管资源，因为中国用户无法可靠访问[cdn.jsdelivr.net](http://cdn.jsdelivr.net/)和[storage.googleapis.com](http://storage.googleapis.com/)。PR #35增加了手掌检测的GPU→CPU降级方案，解决WebGL失败的设备兼容问题。

多模态AI prompt设计很复杂——原始照片加标注照片加关键点测量数据一起发给Claude或Gemini。你不能只说"看看这只手"。prompt有四层结构：角色定义 → 知识注入（五官、三停、十二宫）→ 交叉验证 → 结构化输出格式。

### 积分经济

PR #2（M1 Foundation）：100个文件变更。核心技术挑战是**匿名用户转正式用户的账号合并**。新用户以Supabase匿名用户身份开始。当他们兑换码或登录时，积分、算命记录和聊天历史需要无缝合并。兑换流程是一个状态机：idle → checking → redeeming/logging_in → success/error。

跨标签页积分同步用的是Supabase实时订阅。有一个隐蔽的bug：在一个标签页买了积分，切到另一个标签页，看到旧余额，以为购买失败，再买一次。修复方案：实时订阅在一秒内更新所有标签页。

### 标准化流式传输层

到这个阶段我已经有18条SSE流式路由，每条都有自己的`ReadableStream`实现。PR #16是一次大重构：将15条路由迁移到共享的`createAIStreamResponse()` helper，消除了约1,200行重复样板代码。这个helper增加了生命周期钩子：`initEvents`、`onComplete`、`onError`、`refundCreditsOnError`。

有三条路由确实需要自定义的流式处理：MBTI聊天（accumulated-buffer-delta模式用于信号剥离）、梦境小说（多步骤生成协议）、每日运势（非流式，带服务端缓存）。

### AI语气大改

PR #38由真实用户反馈触发：解读内容太"讨好型"。太积极、太泛泛，不像真正的算命体验。修复方案是对全部6个领域的prompt进行系统性改造：

- 整体语气：从"整体偏积极正面"改为"不要做讨好型解读，好的说好，不好的明确指出"

- 健康解读：从"混合分析"改为"中立偏负面——直接给出健康警示"

- 负面指标：直接使用凶/不利，然后给出化解方案

这是少数几个AI无法独立完成的改动之一。语气校准需要人对文化期望的判断。

## 第四阶段：里程碑与基础设施（第20-29天）

### 里程碑冲刺

2月12日是最高强度的一天：15个PR合并。M1（积分经济）、M2（每日枢纽）、M3（PostHog数据分析）全部上线。

每日枢纽（PR #4）是一个用户粘性设计决策。算命应用天然是事件驱动的——用户带着问题来，得到答案，然后离开。枢纽给了用户每天回来的理由。**关键洞察：通过北京时间午夜的cron任务预生成内容，而不是按需生成。** 用户早上8点打开枢纽时，内容瞬间加载。没有loading。感知上的质量差异非常大。

### 一天上线三个新产品

2月15日：7个PR合并。流年运程功能（PR #85）是一个全新的领域——生肖计算器、太岁分析、流年飞星、AI运势解读。48个文件，162个领域测试。领域知识package本身就包含了星曜目录、星曜年份映射表、太岁规则和生肖性格档案。

### Portal主题bug

PR #67是我最喜欢的工程故事之一。每个应用都有自己的CSS主题（`.theme-hub`、`.theme-tarot`等）。但React portal（弹窗、下拉菜单）通过`createPortal(content, document.body)`渲染——把DOM节点放在了应用主题wrapper的**外面**。作用域在`.theme-hub`上的CSS变量对portal内容是不可见的。

最初的修复（PR #64）是逐组件处理：检测`closest('.theme-hub')`然后硬编码一个`.theme-hub-overlay`类。这在9个主题应用和13个使用portal的组件间无法扩展。

PR #67的方案：一个`ThemeProvider` React context，由app layout声明当前活跃的主题类，加上一个`ThemedPortal`组件，通过MutationObserver用正确的主题类和暗色模式类来包装portal内容。9个app layout更新，13个portal组件迁移。干净、可扩展、正确。

### 弹性AI流式传输

最后一个重大基础设施PR（#105）：服务端流式缓冲。所有19条AI流式路由现在通过`StreamBufferManager`将数据块缓冲到`stream_buffers`数据库表。如果客户端在流式传输中途断开，可以通过`GET /api/stream/{id}`恢复完整响应。统一的`useAIStream` hook替换了14个组件中的3种不一致的SSE消费模式。最终结果：尽管新增了整套弹性基础设施，代码净减186行。Cron任务每15分钟清理过期的缓冲。

### 内容过滤器的演进

内容过滤器经历了三次迭代：

1. **V1（第4天）**：基于DFA的关键词过滤。包含敏感词的消息被整条拦截。

1. **V2（PR #78）**：流感知过滤器。但当敏感词出现时会截断整个流——用户丢失了积分，只得到一个不完整的解读。

1. **V3（PR #104）**：内联遮蔽。敏感词被替换为`**检测到违禁词**`，而不是终止流。双过滤架构：激进的输入过滤器（广告+政治+色情+犯罪）但宽松的输出过滤器（排除"广告"类别，防止对算命术语如兼职、招聘、桑拿的误判）。边界安全的遮蔽处理能应对跨chunk边界的词汇。

## 速度数据

- **1,134个commit**，29天（日均39个，峰值98个）

- **109个PR**，66个合并并附有详细描述

- **9个产品线**，都有真实的业务逻辑

- **85个API端点**，包含19条SSE流式路由

- **约284,000行代码**，分布在16个package中

- **2,021个测试**通过（最终计数）

- **0个零commit日**

共享package是速度的倍增器。新增一个产品线即可免费获得认证、积分、AI模型选择、内容过滤、流式传输、错误处理和UI组件。一个新产品的边际成本几乎全部在于prompt engineering。

在第3篇中，我会分享真实的教训——哪些事让我意外，哪些事我做错了，以及这一切让我对未来的构建方式有了怎样的认识。
